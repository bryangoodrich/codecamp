{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "*Note: You are currently reading this using Google Colaboratory which is a cloud-hosted version of Jupyter Notebook. This is a document containing both text cells for documentation and runnable code cells. If you are unfamiliar with Jupyter Notebook, watch this 3-minute introduction before starting this challenge: https://www.youtube.com/watch?v=inN8seMm7UI*\n",
        "\n",
        "---\n",
        "\n",
        "In this challenge, you need to create a machine learning model that will classify SMS messages as either \"ham\" or \"spam\". A \"ham\" message is a normal message sent by a friend. A \"spam\" message is an advertisement or a message sent by a company.\n",
        "\n",
        "You should create a function called `predict_message` that takes a message string as an argument and returns a list. The first element in the list should be a number between zero and one that indicates the likeliness of \"ham\" (0) or \"spam\" (1). The second element in the list should be the word \"ham\" or \"spam\", depending on which is most likely.\n",
        "\n",
        "For this challenge, you will use the [SMS Spam Collection dataset](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/). The dataset has already been grouped into train data and test data.\n",
        "\n",
        "The first two cells import the libraries and data. The final cell tests your model and function. Add your code in between these cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.12.0-dev20221212\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "lMHwYXHXCar3"
      },
      "outputs": [],
      "source": [
        "train_file_path = \"data/train-data.tsv\"\n",
        "test_file_path = \"data/valid-data.tsv\"\n",
        "header = [\"labels\", \"text\"]\n",
        "\n",
        "train_df = pd.read_csv(train_file_path, sep=\"\\t\", header = None, names = header)\n",
        "test_df = pd.read_csv(test_file_path, sep=\"\\t\", header = None, names = header)\n",
        "\n",
        "train_text, test_text = train_df.text, test_df.text\n",
        "train_labels = np.where(train_df.labels == \"ham\", 0, 1)\n",
        "test_labels = np.where(test_df.labels == \"ham\", 0, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_encoded(tokenizer):\n",
        "    def inner(text, maxlen, truncating = \"post\", padding = \"post\"):\n",
        "        sequences = tokenizer.texts_to_sequences(text)\n",
        "        padded = pad_sequences(sequences, maxlen = maxlen, padding = padding, truncating = truncating)\n",
        "        return padded\n",
        "    return inner\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "# following this this preprocessing and modeling tutorial \n",
        "# https://towardsdatascience.com/nlp-detecting-spam-messages-with-tensorflow-b12195b8cf0e\n",
        "\n",
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "oov_tok = \"<OOV>\"\n",
        "max_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_text)\n",
        "word_index = tokenizer.word_index\n",
        "text_to_sequences = text_encoded(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 100, 16)           16000     \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 30)                48030     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 6)                 186       \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 1)                 7         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 64,223\n",
            "Trainable params: 64,223\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    Flatten(),\n",
        "    Dense(30, activation='relu'),\n",
        "    Dense(6, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "g_h508FEClxO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.0553 - val_accuracy: 0.9878\n",
            "Epoch 2/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.0458 - val_accuracy: 0.9885\n",
            "Epoch 3/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0592 - val_accuracy: 0.9885\n",
            "Epoch 4/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.0625 - val_accuracy: 0.9878\n",
            "Epoch 5/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0598 - val_accuracy: 0.9878\n",
            "Epoch 6/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0524 - val_accuracy: 0.9885\n",
            "Epoch 7/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.0578 - val_accuracy: 0.9878\n",
            "Epoch 8/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0525 - val_accuracy: 0.9885\n",
            "Epoch 9/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0535 - val_accuracy: 0.9885\n",
            "Epoch 10/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0685 - val_accuracy: 0.9885\n",
            "Epoch 11/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0640 - val_accuracy: 0.9885\n",
            "Epoch 12/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0785 - val_accuracy: 0.9856\n",
            "Epoch 13/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0604 - val_accuracy: 0.9885\n",
            "Epoch 14/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0619 - val_accuracy: 0.9892\n",
            "Epoch 15/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 8.6984e-04 - accuracy: 0.9995 - val_loss: 0.0736 - val_accuracy: 0.9885\n",
            "Epoch 16/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0633 - val_accuracy: 0.9878\n",
            "Epoch 17/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0703 - val_accuracy: 0.9878\n",
            "Epoch 18/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0561 - val_accuracy: 0.9878\n",
            "Epoch 19/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 6.9693e-04 - accuracy: 1.0000 - val_loss: 0.0764 - val_accuracy: 0.9871\n",
            "Epoch 20/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0739 - val_accuracy: 0.9871\n",
            "Epoch 21/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0750 - val_accuracy: 0.9871\n",
            "Epoch 22/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 9.9403e-04 - accuracy: 0.9998 - val_loss: 0.0644 - val_accuracy: 0.9892\n",
            "Epoch 23/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 8.7542e-04 - accuracy: 0.9998 - val_loss: 0.0948 - val_accuracy: 0.9864\n",
            "Epoch 24/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 0.0837 - val_accuracy: 0.9864\n",
            "Epoch 25/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0884 - val_accuracy: 0.9878\n",
            "Epoch 26/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0680 - val_accuracy: 0.9878\n",
            "Epoch 27/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 7.9626e-04 - accuracy: 0.9998 - val_loss: 0.0948 - val_accuracy: 0.9856\n",
            "Epoch 28/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0813 - val_accuracy: 0.9864\n",
            "Epoch 29/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0792 - val_accuracy: 0.9864\n",
            "Epoch 30/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 8.9174e-04 - accuracy: 0.9993 - val_loss: 0.1018 - val_accuracy: 0.9856\n",
            "Epoch 31/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0762 - val_accuracy: 0.9878\n",
            "Epoch 32/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 9.8528e-04 - accuracy: 0.9998 - val_loss: 0.0769 - val_accuracy: 0.9871\n",
            "Epoch 33/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 9.8816e-04 - accuracy: 0.9995 - val_loss: 0.0848 - val_accuracy: 0.9864\n",
            "Epoch 34/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 9.5340e-04 - accuracy: 0.9998 - val_loss: 0.0868 - val_accuracy: 0.9864\n",
            "Epoch 35/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 8.9632e-04 - accuracy: 0.9998 - val_loss: 0.0749 - val_accuracy: 0.9885\n",
            "Epoch 36/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 0.0941 - val_accuracy: 0.9849\n",
            "Epoch 37/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0760 - val_accuracy: 0.9885\n",
            "Epoch 38/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 9.7500e-04 - accuracy: 0.9995 - val_loss: 0.1008 - val_accuracy: 0.9849\n",
            "Epoch 39/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0842 - val_accuracy: 0.9864\n",
            "Epoch 40/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 0.0010 - accuracy: 0.9995 - val_loss: 0.0937 - val_accuracy: 0.9864\n",
            "Epoch 41/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0710 - val_accuracy: 0.9892\n",
            "Epoch 42/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 9.2854e-04 - accuracy: 0.9995 - val_loss: 0.0880 - val_accuracy: 0.9871\n",
            "Epoch 43/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 9.1370e-04 - accuracy: 0.9998 - val_loss: 0.0735 - val_accuracy: 0.9892\n",
            "Epoch 44/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 6.3454e-04 - accuracy: 0.9998 - val_loss: 0.0985 - val_accuracy: 0.9856\n",
            "Epoch 45/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0754 - val_accuracy: 0.9885\n",
            "Epoch 46/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0721 - val_accuracy: 0.9849\n",
            "Epoch 47/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.1016 - val_accuracy: 0.9856\n",
            "Epoch 48/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0893 - val_accuracy: 0.9892\n",
            "Epoch 49/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 8.7819e-04 - accuracy: 0.9998 - val_loss: 0.0937 - val_accuracy: 0.9885\n",
            "Epoch 50/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 7.3573e-04 - accuracy: 0.9995 - val_loss: 0.1035 - val_accuracy: 0.9878\n",
            "Epoch 51/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 8.5083e-04 - accuracy: 0.9998 - val_loss: 0.1025 - val_accuracy: 0.9878\n",
            "Epoch 52/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 7.7144e-04 - accuracy: 0.9998 - val_loss: 0.0941 - val_accuracy: 0.9885\n",
            "Epoch 53/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 7.0877e-04 - accuracy: 0.9998 - val_loss: 0.0959 - val_accuracy: 0.9885\n",
            "Epoch 54/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 7.7094e-04 - accuracy: 0.9998 - val_loss: 0.1033 - val_accuracy: 0.9885\n",
            "Epoch 55/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 8.1252e-04 - accuracy: 0.9998 - val_loss: 0.1010 - val_accuracy: 0.9885\n",
            "Epoch 56/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 8.5687e-04 - accuracy: 0.9998 - val_loss: 0.0921 - val_accuracy: 0.9878\n",
            "Epoch 57/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 8.7230e-04 - accuracy: 0.9998 - val_loss: 0.0820 - val_accuracy: 0.9892\n",
            "Epoch 58/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0850 - val_accuracy: 0.9885\n",
            "Epoch 59/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 9.2716e-04 - accuracy: 0.9998 - val_loss: 0.0855 - val_accuracy: 0.9892\n",
            "Epoch 60/200\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0878 - val_accuracy: 0.9885\n",
            "Epoch 61/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 9.6374e-04 - accuracy: 0.9998 - val_loss: 0.0925 - val_accuracy: 0.9885\n",
            "Epoch 62/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 0.1019 - val_accuracy: 0.9856\n",
            "Epoch 63/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 9.2218e-04 - accuracy: 0.9998 - val_loss: 0.0982 - val_accuracy: 0.9885\n",
            "Epoch 64/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 7.9656e-04 - accuracy: 0.9998 - val_loss: 0.1032 - val_accuracy: 0.9871\n",
            "Epoch 65/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 7.3606e-04 - accuracy: 0.9998 - val_loss: 0.0947 - val_accuracy: 0.9885\n",
            "Epoch 66/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 7.1003e-04 - accuracy: 0.9998 - val_loss: 0.1057 - val_accuracy: 0.9871\n",
            "Epoch 67/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 7.0182e-04 - accuracy: 0.9998 - val_loss: 0.0926 - val_accuracy: 0.9878\n",
            "Epoch 68/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 7.6303e-04 - accuracy: 0.9998 - val_loss: 0.1040 - val_accuracy: 0.9871\n",
            "Epoch 69/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 8.8302e-04 - accuracy: 0.9998 - val_loss: 0.0869 - val_accuracy: 0.9878\n",
            "Epoch 70/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0805 - val_accuracy: 0.9878\n",
            "Epoch 71/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 0.0933 - val_accuracy: 0.9871\n",
            "Epoch 72/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 9.7662e-04 - accuracy: 0.9998 - val_loss: 0.0827 - val_accuracy: 0.9878\n",
            "Epoch 73/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 7.8963e-04 - accuracy: 0.9998 - val_loss: 0.0829 - val_accuracy: 0.9871\n",
            "Epoch 74/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 7.2861e-04 - accuracy: 0.9998 - val_loss: 0.1159 - val_accuracy: 0.9871\n",
            "Epoch 75/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 8.8500e-04 - accuracy: 0.9998 - val_loss: 0.0934 - val_accuracy: 0.9878\n",
            "Epoch 76/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 6.6418e-04 - accuracy: 0.9998 - val_loss: 0.1076 - val_accuracy: 0.9871\n",
            "Epoch 77/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 7.8617e-04 - accuracy: 0.9998 - val_loss: 0.1124 - val_accuracy: 0.9871\n",
            "Epoch 78/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 8.6009e-04 - accuracy: 0.9998 - val_loss: 0.0936 - val_accuracy: 0.9878\n",
            "Epoch 79/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 8.5223e-04 - accuracy: 0.9998 - val_loss: 0.0921 - val_accuracy: 0.9885\n",
            "Epoch 80/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 7.9347e-04 - accuracy: 0.9998 - val_loss: 0.0967 - val_accuracy: 0.9864\n",
            "Epoch 81/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 7.8289e-04 - accuracy: 0.9998 - val_loss: 0.0981 - val_accuracy: 0.9871\n",
            "Epoch 82/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 6.5472e-04 - accuracy: 0.9998 - val_loss: 0.1059 - val_accuracy: 0.9871\n",
            "Epoch 83/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 7.6110e-04 - accuracy: 0.9998 - val_loss: 0.0936 - val_accuracy: 0.9871\n",
            "Epoch 84/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 8.2435e-04 - accuracy: 0.9998 - val_loss: 0.0970 - val_accuracy: 0.9871\n",
            "Epoch 85/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 7.1089e-04 - accuracy: 0.9998 - val_loss: 0.1042 - val_accuracy: 0.9871\n",
            "Epoch 86/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 7.1561e-04 - accuracy: 0.9998 - val_loss: 0.1168 - val_accuracy: 0.9864\n",
            "Epoch 87/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 6.8370e-04 - accuracy: 0.9998 - val_loss: 0.1104 - val_accuracy: 0.9871\n",
            "Epoch 88/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 8.2187e-04 - accuracy: 0.9998 - val_loss: 0.0973 - val_accuracy: 0.9871\n",
            "Epoch 89/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 8.6161e-04 - accuracy: 0.9998 - val_loss: 0.1015 - val_accuracy: 0.9871\n",
            "Epoch 90/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 7.0656e-04 - accuracy: 0.9998 - val_loss: 0.1307 - val_accuracy: 0.9864\n",
            "Epoch 91/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 7.3796e-04 - accuracy: 0.9998 - val_loss: 0.1054 - val_accuracy: 0.9878\n",
            "Epoch 92/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 9.5601e-04 - accuracy: 0.9998 - val_loss: 0.1047 - val_accuracy: 0.9871\n",
            "Epoch 93/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 6.9283e-04 - accuracy: 0.9998 - val_loss: 0.1068 - val_accuracy: 0.9871\n",
            "Epoch 94/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 6.6733e-04 - accuracy: 0.9998 - val_loss: 0.1082 - val_accuracy: 0.9871\n",
            "Epoch 95/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 6.3304e-04 - accuracy: 0.9998 - val_loss: 0.1100 - val_accuracy: 0.9871\n",
            "Epoch 96/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 5.9068e-04 - accuracy: 0.9998 - val_loss: 0.1202 - val_accuracy: 0.9878\n",
            "Epoch 97/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 6.7176e-04 - accuracy: 0.9998 - val_loss: 0.1106 - val_accuracy: 0.9878\n",
            "Epoch 98/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 8.4193e-04 - accuracy: 0.9998 - val_loss: 0.1132 - val_accuracy: 0.9871\n",
            "Epoch 99/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 6.4453e-04 - accuracy: 0.9998 - val_loss: 0.1218 - val_accuracy: 0.9871\n",
            "Epoch 100/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 6.7591e-04 - accuracy: 0.9998 - val_loss: 0.1226 - val_accuracy: 0.9871\n",
            "Epoch 101/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 6.3747e-04 - accuracy: 0.9998 - val_loss: 0.1216 - val_accuracy: 0.9871\n",
            "Epoch 102/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 7.1249e-04 - accuracy: 0.9998 - val_loss: 0.1201 - val_accuracy: 0.9871\n",
            "Epoch 103/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 6.9439e-04 - accuracy: 0.9998 - val_loss: 0.1149 - val_accuracy: 0.9878\n",
            "Epoch 104/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 6.2236e-04 - accuracy: 0.9998 - val_loss: 0.1216 - val_accuracy: 0.9871\n",
            "Epoch 105/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 6.2368e-04 - accuracy: 0.9998 - val_loss: 0.1268 - val_accuracy: 0.9871\n",
            "Epoch 106/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 5.8896e-04 - accuracy: 0.9998 - val_loss: 0.1316 - val_accuracy: 0.9871\n",
            "Epoch 107/200\n",
            "131/131 [==============================] - 0s 4ms/step - loss: 6.1327e-04 - accuracy: 0.9998 - val_loss: 0.1390 - val_accuracy: 0.9864\n",
            "Epoch 108/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 6.4225e-04 - accuracy: 0.9998 - val_loss: 0.1259 - val_accuracy: 0.9878\n",
            "Epoch 109/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 9.4516e-04 - accuracy: 0.9998 - val_loss: 0.1133 - val_accuracy: 0.9864\n",
            "Epoch 110/200\n",
            "131/131 [==============================] - 0s 4ms/step - loss: 6.6582e-04 - accuracy: 0.9995 - val_loss: 0.1344 - val_accuracy: 0.9878\n",
            "Epoch 111/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 6.3836e-04 - accuracy: 0.9998 - val_loss: 0.1352 - val_accuracy: 0.9878\n",
            "Epoch 112/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 6.0196e-04 - accuracy: 0.9998 - val_loss: 0.1413 - val_accuracy: 0.9878\n",
            "Epoch 113/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 9.9025e-04 - accuracy: 0.9998 - val_loss: 0.1395 - val_accuracy: 0.9871\n",
            "Epoch 114/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 7.8474e-04 - accuracy: 0.9998 - val_loss: 0.1286 - val_accuracy: 0.9871\n",
            "Epoch 115/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 6.0958e-04 - accuracy: 0.9998 - val_loss: 0.1323 - val_accuracy: 0.9871\n",
            "Epoch 116/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.1491 - val_accuracy: 0.9878\n",
            "Epoch 117/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 6.2943e-04 - accuracy: 0.9998 - val_loss: 0.1332 - val_accuracy: 0.9864\n",
            "Epoch 118/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 6.0559e-04 - accuracy: 0.9998 - val_loss: 0.1498 - val_accuracy: 0.9864\n",
            "Epoch 119/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 6.1640e-04 - accuracy: 0.9998 - val_loss: 0.1426 - val_accuracy: 0.9864\n",
            "Epoch 120/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 8.8634e-04 - accuracy: 0.9998 - val_loss: 0.1578 - val_accuracy: 0.9871\n",
            "Epoch 121/200\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 6.2350e-04 - accuracy: 0.9998 - val_loss: 0.1393 - val_accuracy: 0.9849\n",
            "Epoch 122/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 5.8929e-04 - accuracy: 0.9998 - val_loss: 0.1524 - val_accuracy: 0.9864\n",
            "Epoch 123/200\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 5.8742e-04 - accuracy: 0.9998 - val_loss: 0.1584 - val_accuracy: 0.9871\n",
            "Epoch 124/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 7.7666e-04 - accuracy: 0.9998 - val_loss: 0.1408 - val_accuracy: 0.9878\n",
            "Epoch 125/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 6.8826e-04 - accuracy: 0.9998 - val_loss: 0.1411 - val_accuracy: 0.9878\n",
            "Epoch 126/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.9480e-04 - accuracy: 0.9998 - val_loss: 0.1528 - val_accuracy: 0.9885\n",
            "Epoch 127/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 6.1705e-04 - accuracy: 0.9998 - val_loss: 0.1547 - val_accuracy: 0.9885\n",
            "Epoch 128/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 6.2650e-04 - accuracy: 0.9998 - val_loss: 0.1572 - val_accuracy: 0.9878\n",
            "Epoch 129/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 5.7809e-04 - accuracy: 0.9998 - val_loss: 0.1628 - val_accuracy: 0.9878\n",
            "Epoch 130/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 5.5653e-04 - accuracy: 0.9998 - val_loss: 0.1813 - val_accuracy: 0.9871\n",
            "Epoch 131/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 9.9272e-04 - accuracy: 0.9998 - val_loss: 0.1453 - val_accuracy: 0.9842\n",
            "Epoch 132/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.8269e-04 - accuracy: 0.9998 - val_loss: 0.1596 - val_accuracy: 0.9864\n",
            "Epoch 133/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 6.1207e-04 - accuracy: 0.9998 - val_loss: 0.1656 - val_accuracy: 0.9856\n",
            "Epoch 134/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.7483e-04 - accuracy: 0.9998 - val_loss: 0.1802 - val_accuracy: 0.9849\n",
            "Epoch 135/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.1504 - val_accuracy: 0.9835\n",
            "Epoch 136/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 6.0878e-04 - accuracy: 0.9998 - val_loss: 0.1635 - val_accuracy: 0.9871\n",
            "Epoch 137/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 6.0829e-04 - accuracy: 0.9998 - val_loss: 0.1649 - val_accuracy: 0.9856\n",
            "Epoch 138/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.7283e-04 - accuracy: 0.9998 - val_loss: 0.1735 - val_accuracy: 0.9864\n",
            "Epoch 139/200\n",
            "131/131 [==============================] - 0s 4ms/step - loss: 5.6888e-04 - accuracy: 0.9998 - val_loss: 0.1811 - val_accuracy: 0.9864\n",
            "Epoch 140/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 5.8174e-04 - accuracy: 0.9998 - val_loss: 0.1815 - val_accuracy: 0.9864\n",
            "Epoch 141/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 6.0617e-04 - accuracy: 0.9998 - val_loss: 0.1844 - val_accuracy: 0.9864\n",
            "Epoch 142/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 5.6740e-04 - accuracy: 0.9998 - val_loss: 0.1749 - val_accuracy: 0.9864\n",
            "Epoch 143/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.7176e-04 - accuracy: 0.9998 - val_loss: 0.1799 - val_accuracy: 0.9864\n",
            "Epoch 144/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.5669e-04 - accuracy: 0.9998 - val_loss: 0.1876 - val_accuracy: 0.9864\n",
            "Epoch 145/200\n",
            "131/131 [==============================] - 0s 4ms/step - loss: 5.6097e-04 - accuracy: 0.9998 - val_loss: 0.1894 - val_accuracy: 0.9864\n",
            "Epoch 146/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.8217e-04 - accuracy: 0.9998 - val_loss: 0.1973 - val_accuracy: 0.9849\n",
            "Epoch 147/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.8353e-04 - accuracy: 0.9998 - val_loss: 0.1764 - val_accuracy: 0.9864\n",
            "Epoch 148/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 5.5695e-04 - accuracy: 0.9998 - val_loss: 0.1845 - val_accuracy: 0.9864\n",
            "Epoch 149/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 7.8143e-04 - accuracy: 0.9998 - val_loss: 0.1837 - val_accuracy: 0.9864\n",
            "Epoch 150/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 6.0581e-04 - accuracy: 0.9998 - val_loss: 0.2143 - val_accuracy: 0.9835\n",
            "Epoch 151/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.2103 - val_accuracy: 0.9828\n",
            "Epoch 152/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 8.2911e-04 - accuracy: 0.9998 - val_loss: 0.1784 - val_accuracy: 0.9864\n",
            "Epoch 153/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.2285 - val_accuracy: 0.9842\n",
            "Epoch 154/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 6.2365e-04 - accuracy: 0.9998 - val_loss: 0.2292 - val_accuracy: 0.9849\n",
            "Epoch 155/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 5.5180e-04 - accuracy: 0.9998 - val_loss: 0.2275 - val_accuracy: 0.9856\n",
            "Epoch 156/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 4.7734e-04 - accuracy: 0.9998 - val_loss: 0.2598 - val_accuracy: 0.9849\n",
            "Epoch 157/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 8.1302e-04 - accuracy: 0.9998 - val_loss: 0.1986 - val_accuracy: 0.9849\n",
            "Epoch 158/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 5.6087e-04 - accuracy: 0.9998 - val_loss: 0.2013 - val_accuracy: 0.9856\n",
            "Epoch 159/200\n",
            "131/131 [==============================] - 0s 4ms/step - loss: 5.5889e-04 - accuracy: 0.9998 - val_loss: 0.2033 - val_accuracy: 0.9856\n",
            "Epoch 160/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.5312e-04 - accuracy: 0.9998 - val_loss: 0.2048 - val_accuracy: 0.9856\n",
            "Epoch 161/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 5.5260e-04 - accuracy: 0.9998 - val_loss: 0.2062 - val_accuracy: 0.9864\n",
            "Epoch 162/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 5.5740e-04 - accuracy: 0.9998 - val_loss: 0.2034 - val_accuracy: 0.9856\n",
            "Epoch 163/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 5.5907e-04 - accuracy: 0.9998 - val_loss: 0.2059 - val_accuracy: 0.9864\n",
            "Epoch 164/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 5.5341e-04 - accuracy: 0.9998 - val_loss: 0.2079 - val_accuracy: 0.9871\n",
            "Epoch 165/200\n",
            "131/131 [==============================] - 0s 4ms/step - loss: 5.5052e-04 - accuracy: 0.9998 - val_loss: 0.2074 - val_accuracy: 0.9871\n",
            "Epoch 166/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 5.5072e-04 - accuracy: 0.9998 - val_loss: 0.2092 - val_accuracy: 0.9871\n",
            "Epoch 167/200\n",
            "131/131 [==============================] - 0s 4ms/step - loss: 6.1683e-04 - accuracy: 0.9998 - val_loss: 0.2085 - val_accuracy: 0.9871\n",
            "Epoch 168/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.6346e-04 - accuracy: 0.9998 - val_loss: 0.2050 - val_accuracy: 0.9856\n",
            "Epoch 169/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.5236e-04 - accuracy: 0.9998 - val_loss: 0.2062 - val_accuracy: 0.9864\n",
            "Epoch 170/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.5373e-04 - accuracy: 0.9998 - val_loss: 0.2079 - val_accuracy: 0.9864\n",
            "Epoch 171/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.5323e-04 - accuracy: 0.9998 - val_loss: 0.2099 - val_accuracy: 0.9871\n",
            "Epoch 172/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.5591e-04 - accuracy: 0.9998 - val_loss: 0.2110 - val_accuracy: 0.9871\n",
            "Epoch 173/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.5057e-04 - accuracy: 0.9998 - val_loss: 0.2120 - val_accuracy: 0.9871\n",
            "Epoch 174/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.5022e-04 - accuracy: 0.9998 - val_loss: 0.2135 - val_accuracy: 0.9871\n",
            "Epoch 175/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.5276e-04 - accuracy: 0.9998 - val_loss: 0.2147 - val_accuracy: 0.9871\n",
            "Epoch 176/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.5550e-04 - accuracy: 0.9998 - val_loss: 0.2158 - val_accuracy: 0.9871\n",
            "Epoch 177/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.4408e-04 - accuracy: 0.9998 - val_loss: 0.2164 - val_accuracy: 0.9871\n",
            "Epoch 178/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.5109e-04 - accuracy: 0.9998 - val_loss: 0.2167 - val_accuracy: 0.9871\n",
            "Epoch 179/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.4995e-04 - accuracy: 0.9998 - val_loss: 0.2176 - val_accuracy: 0.9878\n",
            "Epoch 180/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.4737e-04 - accuracy: 0.9998 - val_loss: 0.2197 - val_accuracy: 0.9878\n",
            "Epoch 181/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.7399e-04 - accuracy: 0.9998 - val_loss: 0.2116 - val_accuracy: 0.9849\n",
            "Epoch 182/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 5.9735e-04 - accuracy: 0.9998 - val_loss: 0.2140 - val_accuracy: 0.9849\n",
            "Epoch 183/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 5.5007e-04 - accuracy: 0.9998 - val_loss: 0.2190 - val_accuracy: 0.9871\n",
            "Epoch 184/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 7.7029e-04 - accuracy: 0.9998 - val_loss: 0.2087 - val_accuracy: 0.9828\n",
            "Epoch 185/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 6.0411e-04 - accuracy: 0.9998 - val_loss: 0.2133 - val_accuracy: 0.9835\n",
            "Epoch 186/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 5.6387e-04 - accuracy: 0.9998 - val_loss: 0.2213 - val_accuracy: 0.9849\n",
            "Epoch 187/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 5.6245e-04 - accuracy: 0.9998 - val_loss: 0.2207 - val_accuracy: 0.9849\n",
            "Epoch 188/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 6.3028e-04 - accuracy: 0.9998 - val_loss: 0.2206 - val_accuracy: 0.9835\n",
            "Epoch 189/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 5.5426e-04 - accuracy: 0.9998 - val_loss: 0.2251 - val_accuracy: 0.9849\n",
            "Epoch 190/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 5.5250e-04 - accuracy: 0.9998 - val_loss: 0.2263 - val_accuracy: 0.9856\n",
            "Epoch 191/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 5.5091e-04 - accuracy: 0.9998 - val_loss: 0.2296 - val_accuracy: 0.9856\n",
            "Epoch 192/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 5.4849e-04 - accuracy: 0.9998 - val_loss: 0.2271 - val_accuracy: 0.9849\n",
            "Epoch 193/200\n",
            "131/131 [==============================] - 1s 5ms/step - loss: 5.4818e-04 - accuracy: 0.9998 - val_loss: 0.2305 - val_accuracy: 0.9849\n",
            "Epoch 194/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.5094e-04 - accuracy: 0.9998 - val_loss: 0.2344 - val_accuracy: 0.9856\n",
            "Epoch 195/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.4959e-04 - accuracy: 0.9998 - val_loss: 0.2373 - val_accuracy: 0.9856\n",
            "Epoch 196/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 5.4958e-04 - accuracy: 0.9998 - val_loss: 0.2400 - val_accuracy: 0.9856\n",
            "Epoch 197/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.4742e-04 - accuracy: 0.9998 - val_loss: 0.2420 - val_accuracy: 0.9864\n",
            "Epoch 198/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 5.5005e-04 - accuracy: 0.9998 - val_loss: 0.2508 - val_accuracy: 0.9864\n",
            "Epoch 199/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2219 - val_accuracy: 0.9828\n",
            "Epoch 200/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 6.4673e-04 - accuracy: 0.9998 - val_loss: 0.2227 - val_accuracy: 0.9828\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 200\n",
        "history = model.fit(padded, train_labels, epochs = num_epochs, validation_data = (test_padded, test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add loss plot here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "J9tD9yACG6M9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 40ms/step\n",
            "(4.493818858697374e-11, 'ham')\n"
          ]
        }
      ],
      "source": [
        "# function to predict messages based on model\n",
        "# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n",
        "def predict_message(text):\n",
        "    features = text_to_sequences([text], max_length)\n",
        "    prediction = float(model.predict(features))\n",
        "    return prediction, \"ham\" if prediction < 0.5 else \"spam\"\n",
        "\n",
        "\n",
        "pred_text = \"how are you doing today?\"\n",
        "prediction = predict_message(pred_text)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.000114406444481574, 'ham')"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_message(\"sale today! to stop texts call 98912460324\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Dxotov85SjsC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "You haven't passed yet. Keep trying.\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to test your function and model. Do not modify contents.\n",
        "def test_predictions():\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won Â£1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "  passed = True\n",
        "\n",
        "  for msg, ans in zip(test_messages, test_answers):\n",
        "    prediction = predict_message(msg)\n",
        "    if prediction[1] != ans:\n",
        "      passed = False\n",
        "\n",
        "  if passed:\n",
        "    print(\"You passed the challenge. Great job!\")\n",
        "  else:\n",
        "    print(\"You haven't passed yet. Keep trying.\")\n",
        "\n",
        "test_predictions()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run Once\n",
        "\n",
        "Get the data into a local storage folder\n",
        "\n",
        "Requirement: make a `data` folder here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "urls = [\"https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\", \"https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\"]\n",
        "\n",
        "for url in urls:\n",
        "    filename = os.path.join(\"data\", os.path.basename(url))\n",
        "    with open(filename , 'wb') as fh:\n",
        "        fh.write(requests.get(url).content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "fcc_sms_text_classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "60c36fdf84317347e8cb3e2b8ae1511e6e53331bcde8a306df0845701f1330b3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
